# StudyRAG - AI-Powered Study Assistant

A comprehensive Retrieval-Augmented Generation (RAG) system for intelligent document processing and conversational learning. StudyRAG allows you to upload educational documents and engage in multi-modal conversations with an AI assistant that understands your study materials.

## ğŸŒŸ Features

- **ğŸ“š Multi-Format Document Support**
  - PDF, DOCX, PPTX, TXT files
  - Image OCR support (PNG, JPG, JPEG)
  - Automatic text extraction and chunking

- **ğŸ¤– Intelligent RAG Engine**
  - Vector-based semantic search using FAISS
  - Retrieves complete document context for answers
  - Token-based text chunking (1024 tokens with 12.5% overlap)
  - Top-K results (configurable, retrieves all chunks by default)

- **ğŸ’¬ Multi-Mode Conversations**
  - Chat mode: General discussion
  - Explain mode: Detailed explanations
  - Summary mode: Quick summaries
  - Points mode: Key takeaways
  - Flashcards mode: Review flashcards
  - Exam mode: Practice questions

- **ğŸ’¾ Session Management**
  - Persistent chat history (stored as JSON)
  - Per-session vector stores
  - Upload tracking and management
  - Automatic history recovery on refresh

- **ğŸ¨ Modern Web Interface**
  - Dark theme UI
  - Real-time file upload with progress
  - PDF export of conversations
  - Responsive design
  - Custom scrollbar styling

- **âš™ï¸ Advanced Configuration**
  - GPU support (CUDA/CPU/MPS)
  - FP16/FP32 precision control for Whisper
  - Configurable retrieval strategy
  - Environment-based settings

## ğŸ—ï¸ Architecture

```
StudyRAG/
â”œâ”€â”€ frontend/              # Web UI (HTML/CSS/JS)
â”‚   â”œâ”€â”€ index.html        # Session management page
â”‚   â”œâ”€â”€ chat.html         # Chat interface
â”‚   â”œâ”€â”€ style.css         # Dark theme styling
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ api.js        # API client
â”‚   â”‚   â”œâ”€â”€ chat.js       # Chat interactions
â”‚   â”‚   â””â”€â”€ sessions.js   # Session management
â”‚   â”œâ”€â”€ Dockerfile        # Frontend container
â”‚   â””â”€â”€ nginx.conf        # Nginx reverse proxy config
â”‚
â”œâ”€â”€ backend/              # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py       # FastAPI routes
â”‚   â”‚   â”œâ”€â”€ config.py     # Configuration settings
â”‚   â”‚   â”œâ”€â”€ db.py         # SQLite database layer
â”‚   â”‚   â”œâ”€â”€ models.py     # Pydantic models
â”‚   â”‚   â”œâ”€â”€ processors/   # Document processing
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf.py
â”‚   â”‚   â”‚   â”œâ”€â”€ docx.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pptx.py
â”‚   â”‚   â”‚   â””â”€â”€ ocr.py
â”‚   â”‚   â””â”€â”€ rag/          # RAG pipeline
â”‚   â”‚       â”œâ”€â”€ chat.py   # Chatbot logic
â”‚   â”‚       â”œâ”€â”€ loader.py # Document ingestion
â”‚   â”‚       â”œâ”€â”€ chunker.py # Text chunking
â”‚   â”‚       â”œâ”€â”€ embedder.py # Embedding generation
â”‚   â”‚       â”œâ”€â”€ retriever.py # Vector search
â”‚   â”‚       â””â”€â”€ prompts.py # Mode-based prompts
â”‚   â”œâ”€â”€ data/             # Data storage
â”‚   â”‚   â”œâ”€â”€ vectors/      # FAISS indices
â”‚   â”‚   â”œâ”€â”€ history/      # Chat histories
â”‚   â”‚   â””â”€â”€ studyrag.db   # SQLite database
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .dockerignore
â”‚
â”œâ”€â”€ docker-compose.yml    # Complete stack orchestration
â””â”€â”€ README.md
```

## ğŸ“‹ Prerequisites

### Local Development
- Python 3.11+
- Ollama (for LLM inference)
- CUDA/GPU support (optional, for faster Whisper transcription)
- FFmpeg (for audio processing)
- Tesseract OCR (for image text extraction)

### Docker (Recommended)
- Docker 20.10+
- Docker Compose 2.0+
- 8GB+ RAM recommended

## ğŸš€ Quick Start

### Option 1: Docker (Easiest)

```bash
# Clone the repository
git clone https://github.com/zorocancode/StudyRAG.git
cd StudyRAG

# Start all services
docker-compose up -d

# On first run, pull the required Ollama models
docker exec ollama-server ollama pull llama3:latest
docker exec ollama-server ollama pull nomic-embed-text

# Access the application
# Frontend: http://localhost
# Backend API: http://localhost:8000/docs
```

### Option 2: Local Development

**Backend Setup:**
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/Scripts/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start Ollama (in separate terminal)
ollama serve

# Pull the required models
ollama pull llama3:latest
ollama pull nomic-embed-text

# Run the backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Frontend Setup:**
```bash
cd frontend

# If using a web server (e.g., Python)
python -m http.server 8080

# Or open directly in browser
# file:///path/to/StudyRAG/frontend/index.html
```

## âš™ï¸ Configuration

Edit `backend/app/config.py`:

```python
# LLM Settings
OLLAMA_MODEL = "llama3:latest"
OLLAMA_BASE_URL = "http://localhost:11434"
TEMPERATURE = 0.7

# RAG Settings
CHUNK_SIZE = 1024  # tokens
CHUNK_OVERLAP_PERCENT = 0.125  # 12.5%
EMBEDDING_MODEL = "nomic-embed-text"  # Ollama embedding model
TOP_K_RESULTS = 1000
ALL_RESULTS = True  # Retrieve all chunks by default

# Whisper Settings
WHISPER_MODEL = "base"  # tiny, base, small, medium, large
WHISPER_FP16 = False  # Use FP32 for stability
WHISPER_DEVICE = "cuda"  # cuda, cpu, mps

# Document Settings
MAX_UPLOAD_SIZE = 100 * 1024 * 1024  # 100MB
SUPPORTED_FORMATS = ["pdf", "docx", "pptx", "txt", "jpg", "png", "jpeg"]
```

## ğŸ“¡ API Endpoints

### Sessions
- `GET /sessions` - List all sessions
- `POST /sessions` - Create new session
- `GET /sessions/{id}` - Get session details
- `DELETE /sessions/{id}` - Delete session

### Documents
- `POST /upload` - Upload document
- `GET /sessions/{id}/files` - List uploaded files
- `DELETE /sessions/{id}/files/{file_id}` - Delete file

### Chat
- `POST /chat` - Send message (RAG-enabled)
- `GET /sessions/{id}/history` - Get chat history

### System
- `GET /docs` - Swagger API documentation
- `GET /health` - Health check

## ğŸ³ Docker Hub

Pre-built images available on Docker Hub:

```bash
# Using Docker Hub images
docker pull zorocancode/studyrag-backend:latest
docker pull zorocancode/studyrag-frontend:latest

# Run with docker-compose
docker-compose up -d
```

## ğŸ“Š Data Storage

- **SQLite Database** (`data/studyrag.db`)
  - Sessions metadata
  - Upload records
  - User preferences

- **FAISS Indices** (`data/vectors/`)
  - Per-session vector embeddings
  - Binary index files (.index)
  - Metadata pickles (.meta)

- **Chat History** (`data/history/`)
  - JSON files per session
  - Conversation records with sources

## ğŸ”§ Troubleshooting

### Backend Issues

**CUDA not available:**
```bash
# Reinstall PyTorch with CUDA support
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Ollama connection error:**
- Ensure Ollama is running: `ollama serve`
- Check OLLAMA_BASE_URL in config
- Verify network connectivity in Docker

**Out of memory:**
- Reduce CHUNK_SIZE
- Use smaller WHISPER_MODEL (tiny or base)
- Reduce MAX_UPLOAD_SIZE

### Frontend Issues

**API calls failing:**
- Check backend is running on port 8000
- Verify nginx proxy config (frontend/nginx.conf)
- Check CORS headers in FastAPI

**PDF export not working:**
- Ensure html2pdf library is loaded
- Check browser console for errors
- Try a different browser

## ğŸ¯ Usage Examples

### 1. Create a Study Session
1. Open http://localhost
2. Click "Create Session"
3. Enter session name and subject

### 2. Upload Documents
1. Navigate to session
2. Click "ğŸ“ Upload"
3. Select PDF, DOCX, or image files
4. Documents are automatically processed

### 3. Ask Questions
1. Type your question in the chat
2. Select a mode (Chat, Explain, Summary, etc.)
3. AI searches documents and provides answers with sources

### 4. Export Conversation
1. Click "ğŸ“„ Export PDF"
2. Conversation is downloaded with formatting

## ğŸŒ Environment Variables

Create `.env` file in backend/:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
WHISPER_DEVICE=cuda
DEBUG=False
```

## ğŸ“ˆ Performance Tips

1. **Use GPU for Whisper:** Set `WHISPER_DEVICE=cuda` (requires CUDA PyTorch)
2. **Adjust chunk size:** Larger chunks = faster but less precise
3. **Limit TOP_K_RESULTS:** Smaller values = faster responses
4. **Use medium Whisper model:** Balance between speed and accuracy
5. **Enable GZIP compression:** Already configured in nginx

## ğŸ› Known Limitations

- YouTube ingestion removed (use alternative tools)
- Single Ollama model per deployment
- Vector indices stored locally (not distributed)
- SQLite database (suitable for small to medium deployments)

## ğŸš€ Future Enhancements

- [ ] Multi-LLM support (GPT-4, Claude)
- [ ] Distributed vector store (Weaviate, Milvus)
- [ ] Real-time collaboration
- [ ] Advanced search filters
- [ ] Custom prompt templates
- [ ] Usage analytics dashboard
- [ ] Multiple language support
- [ ] Fine-tuning on custom datasets

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ‘¤ Author

**zorocancode** - [Docker Hub](https://hub.docker.com/r/zorocancode)

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“ Support

For issues and questions:
- Check [Troubleshooting](#-troubleshooting) section
- Review API docs at `/docs` endpoint
- Check Docker logs: `docker-compose logs -f`

## ğŸ™ Acknowledgments

- OpenAI Whisper for transcription
- Ollama for local LLM inference
- LangChain for RAG framework
- FAISS for vector similarity search
- FastAPI for the backend framework
- Nginx for reverse proxy

---

**Happy Learning! ğŸ“š**